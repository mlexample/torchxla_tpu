{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6cb7e38-5292-4858-88ea-b5b599be37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "from pprint import pprint\n",
    "import random\n",
    "import time\n",
    "from itertools import cycle, islice, chain\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dbce1f-e0a4-4844-8289-873f1037ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='hybrid-vertex'\n",
    "\n",
    "# storage client\n",
    "storage_client = storage.Client(\n",
    "    project=PROJECT_ID\n",
    ")\n",
    "\n",
    "train_files = [\n",
    "    'gs://imagenet-jt/train/train-00000-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00001-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00002-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00003-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00004-of-01024',\n",
    "]\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "options = tf.data.Options()\n",
    "options.experimental_deterministic = False\n",
    "# options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO # FILE | DATA | AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3ff45-5eb6-4618-ae7d-179e006d8051",
   "metadata": {},
   "source": [
    "## MyIterable dataset\n",
    "\n",
    "> Partition data into groups, feed each group into a single stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966dbe4d-7611-4a2f-af0b-bf7313f7c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, data_list, batch_size):\n",
    "        self.data_list = data_list\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    @property\n",
    "    def shuffled_data_list(self):\n",
    "        return random.sample(self.data_list, len(self.data_list))\n",
    "    \n",
    "    def process_data(self, data):\n",
    "        for x in data:\n",
    "            yield x\n",
    "    \n",
    "    def get_stream(self, data_list):\n",
    "        return chain.from_iterable(map(self.process_data, cycle(data_list)))\n",
    "    \n",
    "    def get_streams(self):\n",
    "        return zip(*[self.get_stream(self.shuffled_data_list) for _ in range(self.batch_size)])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb07aab-0d1b-4866-9791-aad582e9d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = [\n",
    "    [12, 13, 14, 15, 16, 17],\n",
    "    [27, 28, 29],\n",
    "    [31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
    "    [40, 41, 42, 43],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c50a55-6f2f-4c17-a2e5-98e364b58ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 40, 31, 31]\n",
      "[41, 41, 32, 32]\n",
      "[42, 42, 33, 33]\n",
      "[43, 43, 34, 34]\n",
      "[31, 27, 35, 35]\n",
      "[32, 28, 36, 36]\n",
      "[33, 29, 37, 37]\n",
      "[34, 12, 38, 38]\n",
      "[35, 13, 39, 39]\n",
      "[36, 14, 40, 27]\n",
      "[37, 15, 41, 28]\n",
      "[38, 16, 42, 29]\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = MyIterableDataset(synthetic_data, batch_size=4)\n",
    "\n",
    "loader = DataLoader(iterable_dataset, batch_size=None)\n",
    "\n",
    "for batch in islice(loader, 12):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640e658-ab6c-4c2f-9dbf-eea527ec47d5",
   "metadata": {},
   "source": [
    "## MyIterable ds TFRecords to torch_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca2217-92bd-44bb-a219-f8ed40240d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1f779-0b32-44fd-87ab-cc1d74a49d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterable_tf_Dataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, filenames, batch_size, length):\n",
    "        self.filenames = filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        \n",
    "    def __len__(self, length):\n",
    "        '''\n",
    "        Needed for torch dataloader\n",
    "        '''\n",
    "        return self.length\n",
    "    \n",
    "    def process_data(self, data):\n",
    "        for x in data:\n",
    "            yield x\n",
    "    \n",
    "    def identity(x):\n",
    "        return x\n",
    "    \n",
    "    # @classmethod\n",
    "    def split_tfrecords_per_node(filenames):\n",
    "        \"\"\"\n",
    "        Split TFRecords correctly per accelerator node\n",
    "        :param filenames:\n",
    "        :return: slice of filenames\n",
    "        \"\"\"\n",
    "        rank=xm.get_ordinal()\n",
    "        num_replicas=xm.xrt_world_size()\n",
    "        filenames_this = filenames[rank::num_replicas]\n",
    "        \n",
    "        return filenames_this\n",
    "    \n",
    "    def tfrecords_per_worker(filenames):\n",
    "        \"\"\"\n",
    "        Split filenames per worker\n",
    "        Selects a subset of filenames based on Torch get_worker_info.\n",
    "        Used as a shard selection function in Dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        filenames = [file for file in filenames]\n",
    "\n",
    "        assert isinstance(filenames, list)\n",
    "\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            wid = worker_info.id\n",
    "            num_workers = worker_info.num_workers\n",
    "\n",
    "            return filenames[wid::num_workers]\n",
    "        else:\n",
    "            return filenames\n",
    "    \n",
    "    def tfrecord_dataset(ds):\n",
    "        buffer_size = 8 * 1024 * 1024 # 8 MiB per file\n",
    "        return tf.data.TFRecordDataset(ds, buffer_size=buffer_size)\n",
    "        \n",
    "    def parse_tfrecord(self, example):\n",
    "        \n",
    "        feature_map = {\n",
    "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        \n",
    "        parsed = tf.io.parse_example(example, feature_map)\n",
    "    \n",
    "        label = parsed['image/class/label']  \n",
    "        raw_img = parsed['image/encoded']\n",
    "\n",
    "        # TODO: make this a function (?)\n",
    "        img = tf.io.decode_jpeg(raw_img)                        # (240, 320, 3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [224, 224])                  # (224, 224, 3)\n",
    "\n",
    "        transposed_tf_tensor = tf.transpose(img, perm=[2, 0, 1])  # (224, 224, 3) -> (3, 224, 224)\n",
    "        \n",
    "        return label, transposed_tf_tensor\n",
    "    \n",
    "    ds = ds.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False,\n",
    "    ).map(\n",
    "        parse_tfrecord,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE, # parallelize across many cores\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c19ae-e374-4194-9f7d-4ee5e65829a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob filename from GCS\n",
    "# for blob in storage_client.list_blobs(f'{self.train_dir}', prefix=f'{self.train_dir_prefix}', delimiter=\"/\"):\n",
    "    # train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\")) #\"/gcs/\"\n",
    "\n",
    "# for testing\n",
    "train_files = [\n",
    "    'gs://imagenet-jt/train/train-00000-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00001-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00002-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00003-of-01024',\n",
    "    # 'gs://imagenet-jt/train/train-00004-of-01024',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba42f4-59ed-4065-950e-c778eb2a8c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26db916-e095-42c6-8ea1-7fae40ada588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd089a9-43be-44b6-9f88-52027a0cd53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9553c2-6f43-4df9-800a-890c9a4b7c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
